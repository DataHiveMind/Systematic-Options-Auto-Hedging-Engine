{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6831c146",
   "metadata": {},
   "source": [
    "# Exploration Data Analysis Notebook\n",
    "The purpose of this notebook is to load, preprocess and exploratory raw data from OpenBB Terimnal\n",
    "\n",
    "## Key Activites:\n",
    "    1. Load Raw data from openbb using src/hedging_engine/data_loader.py\n",
    "    2. Explore the data for volatility and monutnem\n",
    "    3. Save processed data to data/interim for more feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a84c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspaces/Systematic-Options-Auto-Hedging-Engine\n",
      "Source path: /workspaces/Systematic-Options-Auto-Hedging-Engine/src\n",
      "Python path updated successfully!\n",
      "Project root exists: True\n",
      "Source path exists: True\n",
      "Data loader exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Get the project root directory (one level up from notebooks folder)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Add both project root and src to Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Source path: {src_path}\")\n",
    "print(f\"Python path updated successfully!\")\n",
    "\n",
    "# Verify the paths exist\n",
    "print(f\"Project root exists: {os.path.exists(project_root)}\")\n",
    "print(f\"Source path exists: {os.path.exists(src_path)}\")\n",
    "print(f\"Data loader exists: {os.path.exists(os.path.join(src_path, 'hedging_engine', 'data_loader.py'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a6269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully imported data_loader from hedging_engine\n"
     ]
    }
   ],
   "source": [
    "# Import the data_loader module\n",
    "try:\n",
    "    from hedging_engine import data_loader\n",
    "    print(\"âœ“ Successfully imported data_loader from hedging_engine\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Failed to import from hedging_engine: {e}\")\n",
    "    try:\n",
    "        # Alternative import method\n",
    "        import hedging_engine.data_loader as data_loader\n",
    "        import hedging_engine.save_raw_data as save_raw_data\n",
    "        print(\"âœ“ Successfully imported data_loader using alternative method\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"âœ— Alternative import also failed: {e2}\")\n",
    "        # Direct import as last resort\n",
    "        sys.path.append(os.path.join(src_path, 'hedging_engine'))\n",
    "        import data_loader\n",
    "        print(\"âœ“ Successfully imported data_loader using direct method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a37b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Matplotlib backend: Agg\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend to prevent issues\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style (using more compatible style)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')  # Fallback to default style\n",
    "    \n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for notebook\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca42235",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "Load historical stock data using the updated data_loader module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9385bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data...\n",
      "Tickers: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
      "Date range: 2020-01-01 to 2023-12-31\n",
      "\n",
      "Loading data for AAPL...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/AAPL_raw_20251001.csv\n",
      "Input data shape: (1006, 7)\n",
      "Final preprocessed data shape: (1006, 7)\n",
      "âœ“ Successfully loaded and saved 1006 records for AAPL\n",
      "\n",
      "Loading data for MSFT...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/AAPL_raw_20251001.csv\n",
      "Input data shape: (1006, 7)\n",
      "Final preprocessed data shape: (1006, 7)\n",
      "âœ“ Successfully loaded and saved 1006 records for AAPL\n",
      "\n",
      "Loading data for MSFT...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/MSFT_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for MSFT\n",
      "\n",
      "Loading data for GOOGL...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/MSFT_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for MSFT\n",
      "\n",
      "Loading data for GOOGL...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/GOOGL_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for GOOGL\n",
      "\n",
      "Loading data for AMZN...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/GOOGL_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for GOOGL\n",
      "\n",
      "Loading data for AMZN...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/AMZN_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for AMZN\n",
      "\n",
      "Loading data for TSLA...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/AMZN_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for AMZN\n",
      "\n",
      "Loading data for TSLA...\n",
      "Attempting to load data with provider: yfinance\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/TSLA_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for TSLA\n",
      "\n",
      "Successfully loaded data for 5 stocks\n",
      "Successfully loaded data with provider: yfinance\n",
      "Raw data saved to /workspaces/Systematic-Options-Auto-Hedging-Engine/data/raw/TSLA_raw_20251001.csv\n",
      "Input data shape: (1006, 6)\n",
      "Final preprocessed data shape: (1006, 6)\n",
      "âœ“ Successfully loaded and saved 1006 records for TSLA\n",
      "\n",
      "Successfully loaded data for 5 stocks\n"
     ]
    }
   ],
   "source": [
    "# Load historical data for analysis\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"]  # Multiple stocks for analysis\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "print(\"Loading historical data...\")\n",
    "print(f\"Tickers: {tickers}\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "\n",
    "# Create directories for saving data\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Load data for each ticker\n",
    "stock_data = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(f\"\\nLoading data for {ticker}...\")\n",
    "        \n",
    "        # Load raw data\n",
    "        df_raw = data_loader.load_data(ticker, start_date, end_date)\n",
    "        \n",
    "        # Save raw data before processing\n",
    "        raw_filename = os.path.join(raw_dir, f\"{ticker}_raw_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "        data_loader.save_raw_data(df_raw, raw_filename)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df_processed = data_loader.preprocess_data(df_raw)\n",
    "        stock_data[ticker] = df_processed\n",
    "        \n",
    "        print(f\"âœ“ Successfully loaded and saved {len(df_processed)} records for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to load {ticker}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded data for {len(stock_data)} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f88bda",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Basic Statistics\n",
    "Examine the structure and basic properties of the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fe6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA OVERVIEW FOR AAPL\n",
      "==================================================\n",
      "Shape: (1006, 7)\n",
      "Date range: 2020-01-02 to 2023-12-29\n",
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'split_ratio', 'dividend']\n",
      "\n",
      "Basic Statistics:\n",
      "              open         high          low        close        volume\n",
      "count  1006.000000  1006.000000  1006.000000  1006.000000  1.006000e+03\n",
      "mean    140.675507   142.321389   139.143536   140.808131  9.895373e+07\n",
      "std      33.310018    33.430571    33.179199    33.313857  5.439610e+07\n",
      "min      57.020000    57.125000    53.152500    56.092499  2.404830e+07\n",
      "25%     123.682503   125.030003   122.157499   123.592501  6.407675e+07\n",
      "50%     145.540001   147.264999   144.120003   145.860001  8.467540e+07\n",
      "75%     166.302498   168.147503   164.815002   166.214996  1.155069e+08\n",
      "max     198.020004   199.619995   197.000000   198.110001  4.265100e+08\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "==================================================\n",
      "DATA OVERVIEW FOR MSFT\n",
      "==================================================\n",
      "Shape: (1006, 6)\n",
      "Date range: 2020-01-02 to 2023-12-29\n",
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'dividend']\n",
      "\n",
      "Basic Statistics:\n",
      "              open         high          low        close        volume\n",
      "count  1006.000000  1006.000000  1006.000000  1006.000000  1.006000e+03\n",
      "mean    262.669344   265.509354   259.803986   262.781928  3.065455e+07\n",
      "std      54.652568    54.732373    54.415635    54.621965  1.299770e+07\n",
      "min     137.009995   140.570007   132.520004   135.419998  9.200800e+06\n",
      "25%     222.002502   224.144997   219.130005   221.469994  2.249522e+07\n",
      "50%     259.165009   261.500000   255.989998   259.464996  2.727275e+07\n",
      "75%     304.230003   307.820000   301.120010   304.322487  3.462020e+07\n",
      "max     383.760010   384.299988   378.160004   382.700012  9.701270e+07\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "==================================================\n",
      "DATA OVERVIEW FOR GOOGL\n",
      "==================================================\n",
      "Shape: (1006, 6)\n",
      "Date range: 2020-01-02 to 2023-12-29\n",
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'split_ratio']\n",
      "\n",
      "Basic Statistics:\n",
      "              open         high          low        close        volume\n",
      "count  1006.000000  1006.000000  1006.000000  1006.000000  1.006000e+03\n",
      "mean    107.802233   109.088274   106.595380   107.870246  3.449953e+07\n",
      "std      25.168280    25.275266    24.983858    25.114855  1.501422e+07\n",
      "min      52.818501    53.345501    50.443501    52.706501  9.312000e+06\n",
      "25%      88.102499    88.856625    86.948874    88.028498  2.502685e+07\n",
      "50%     109.788750   111.436501   107.970001   109.888500  3.071800e+07\n",
      "75%     130.753246   132.082123   128.990002   130.250000  3.933900e+07\n",
      "max     151.250000   151.546494   148.899002   149.838501  1.232000e+08\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "==================================================\n",
      "DATA OVERVIEW FOR AMZN\n",
      "==================================================\n",
      "Shape: (1006, 6)\n",
      "Date range: 2020-01-02 to 2023-12-29\n",
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'split_ratio']\n",
      "\n",
      "Basic Statistics:\n",
      "              open         high          low        close        volume\n",
      "count  1006.000000  1006.000000  1006.000000  1006.000000  1.006000e+03\n",
      "mean    137.255245   139.034453   135.376526   137.216247  7.538060e+07\n",
      "std      27.576463    27.607319    27.413947    27.468805  3.382786e+07\n",
      "min      82.075500    83.480003    81.301498    81.820000  2.237840e+07\n",
      "25%     113.835001   116.115000   112.434999   114.309002  5.278502e+07\n",
      "50%     140.830254   143.375000   138.939751   140.585007  6.577000e+07\n",
      "75%     161.148872   162.888756   159.206749   161.190620  8.729165e+07\n",
      "max     187.199997   188.654007   184.839493   186.570496  3.113460e+08\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "==================================================\n",
      "DATA OVERVIEW FOR TSLA\n",
      "==================================================\n",
      "Shape: (1006, 6)\n",
      "Date range: 2020-01-02 to 2023-12-29\n",
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'split_ratio']\n",
      "\n",
      "Basic Statistics:\n",
      "              open         high          low        close        volume\n",
      "count  1006.000000  1006.000000  1006.000000  1006.000000  1.006000e+03\n",
      "mean    209.199484   213.949515   204.067199   209.126371  1.332264e+08\n",
      "std      85.988785    87.686367    83.987942    85.797682  8.862913e+07\n",
      "min      24.980000    26.990667    23.367332    24.081333  2.940180e+07\n",
      "25%     159.702499   161.952496   153.649998   160.210003  7.707188e+07\n",
      "50%     223.954994   229.125000   218.174995   223.489998  1.071037e+08\n",
      "75%     263.557503   268.260010   258.307510   262.967491  1.578315e+08\n",
      "max     411.470001   414.496674   405.666656   409.970001  9.140820e+08\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "# Examine data structure and basic statistics\n",
    "for ticker, df in stock_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATA OVERVIEW FOR {ticker}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(df[['open', 'high', 'low', 'close', 'volume']].describe())\n",
    "    \n",
    "    print(f\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0] if missing.sum() > 0 else \"No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded844df",
   "metadata": {},
   "source": [
    "## 3. Price Movement Analysis\n",
    "Analyze price movements, returns, and basic momentum indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "850b9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AAPL - Return Statistics:\n",
      "Daily Return - Mean: 0.0012, Std: 0.0211\n",
      "Annualized Volatility: 0.1441\n",
      "10-day Momentum: -0.0282\n",
      "20-day Momentum: 0.0136\n",
      "\n",
      "MSFT - Return Statistics:\n",
      "Daily Return - Mean: 0.0011, Std: 0.0206\n",
      "Annualized Volatility: 0.1462\n",
      "10-day Momentum: 0.0276\n",
      "20-day Momentum: -0.0076\n",
      "\n",
      "GOOGL - Return Statistics:\n",
      "Daily Return - Mean: 0.0009, Std: 0.0211\n",
      "Annualized Volatility: 0.2549\n",
      "10-day Momentum: 0.0587\n",
      "20-day Momentum: 0.0540\n",
      "\n",
      "AMZN - Return Statistics:\n",
      "Daily Return - Mean: 0.0007, Std: 0.0237\n",
      "Annualized Volatility: 0.1896\n",
      "10-day Momentum: 0.0307\n",
      "20-day Momentum: 0.0400\n",
      "\n",
      "TSLA - Return Statistics:\n",
      "Daily Return - Mean: 0.0031, Std: 0.0429\n",
      "Annualized Volatility: 0.3345\n",
      "10-day Momentum: -0.0102\n",
      "20-day Momentum: 0.0350\n"
     ]
    }
   ],
   "source": [
    "# Calculate returns and basic momentum indicators\n",
    "enhanced_data = {}\n",
    "\n",
    "for ticker, df in stock_data.items():\n",
    "    # Create a copy for feature engineering\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    enhanced_df['daily_return'] = enhanced_df['close'].pct_change()\n",
    "    enhanced_df['log_return'] = np.log(enhanced_df['close'] / enhanced_df['close'].shift(1))\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    enhanced_df['ma_5'] = enhanced_df['close'].rolling(window=5).mean()\n",
    "    enhanced_df['ma_20'] = enhanced_df['close'].rolling(window=20).mean()\n",
    "    enhanced_df['ma_50'] = enhanced_df['close'].rolling(window=50).mean()\n",
    "    \n",
    "    # Calculate volatility (rolling 20-day)\n",
    "    enhanced_df['volatility_20'] = enhanced_df['daily_return'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Calculate momentum indicators\n",
    "    enhanced_df['momentum_10'] = enhanced_df['close'] / enhanced_df['close'].shift(10) - 1\n",
    "    enhanced_df['momentum_20'] = enhanced_df['close'] / enhanced_df['close'].shift(20) - 1\n",
    "    \n",
    "    # Calculate price range and true range\n",
    "    enhanced_df['price_range'] = (enhanced_df['high'] - enhanced_df['low']) / enhanced_df['close']\n",
    "    enhanced_df['true_range'] = np.maximum(\n",
    "        enhanced_df['high'] - enhanced_df['low'],\n",
    "        np.maximum(\n",
    "            abs(enhanced_df['high'] - enhanced_df['close'].shift(1)),\n",
    "            abs(enhanced_df['low'] - enhanced_df['close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    enhanced_data[ticker] = enhanced_df\n",
    "    \n",
    "    # Print summary statistics for returns\n",
    "    print(f\"\\n{ticker} - Return Statistics:\")\n",
    "    print(f\"Daily Return - Mean: {enhanced_df['daily_return'].mean():.4f}, Std: {enhanced_df['daily_return'].std():.4f}\")\n",
    "    print(f\"Annualized Volatility: {enhanced_df['volatility_20'].iloc[-1]:.4f}\")\n",
    "    print(f\"10-day Momentum: {enhanced_df['momentum_10'].iloc[-1]:.4f}\")\n",
    "    print(f\"20-day Momentum: {enhanced_df['momentum_20'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d0865",
   "metadata": {},
   "source": [
    "## 4. Volatility Analysis\n",
    "Detailed analysis of volatility patterns and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f59457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Volatility Statistics Summary:\n",
      "============================================================\n",
      "\n",
      "AAPL:\n",
      "  Mean Volatility: 0.3052\n",
      "  Min Volatility:  0.1202\n",
      "  Max Volatility:  1.0795\n",
      "  Std Volatility:  0.1498\n",
      "\n",
      "MSFT:\n",
      "  Mean Volatility: 0.2974\n",
      "  Min Volatility:  0.1133\n",
      "  Max Volatility:  1.1314\n",
      "  Std Volatility:  0.1477\n",
      "\n",
      "GOOGL:\n",
      "  Mean Volatility: 0.3158\n",
      "  Min Volatility:  0.1210\n",
      "  Max Volatility:  0.8958\n",
      "  Std Volatility:  0.1250\n",
      "\n",
      "AMZN:\n",
      "  Mean Volatility: 0.3561\n",
      "  Min Volatility:  0.1372\n",
      "  Max Volatility:  0.7411\n",
      "  Std Volatility:  0.1344\n",
      "\n",
      "TSLA:\n",
      "  Mean Volatility: 0.6327\n",
      "  Min Volatility:  0.2094\n",
      "  Max Volatility:  1.5917\n",
      "  Std Volatility:  0.2454\n"
     ]
    }
   ],
   "source": [
    "# Analyze volatility patterns\n",
    "fig, axes = plt.subplots(len(enhanced_data), 2, figsize=(15, 4*len(enhanced_data)))\n",
    "if len(enhanced_data) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (ticker, df) in enumerate(enhanced_data.items()):\n",
    "    # Plot 1: Price and Moving Averages\n",
    "    axes[i, 0].plot(df.index, df['close'], label='Close Price', alpha=0.7)\n",
    "    axes[i, 0].plot(df.index, df['ma_20'], label='MA 20', alpha=0.8)\n",
    "    axes[i, 0].plot(df.index, df['ma_50'], label='MA 50', alpha=0.8)\n",
    "    axes[i, 0].set_title(f'{ticker} - Price and Moving Averages')\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Volatility over time\n",
    "    axes[i, 1].plot(df.index, df['volatility_20'], label='20-day Volatility', color='red', alpha=0.7)\n",
    "    axes[i, 1].set_title(f'{ticker} - Rolling 20-day Volatility')\n",
    "    axes[i, 1].set_ylabel('Annualized Volatility')\n",
    "    axes[i, 1].legend()\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Volatility statistics\n",
    "print(\"\\nVolatility Statistics Summary:\")\n",
    "print(\"=\"*60)\n",
    "for ticker, df in enhanced_data.items():\n",
    "    vol_stats = df['volatility_20'].describe()\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Mean Volatility: {vol_stats['mean']:.4f}\")\n",
    "    print(f\"  Min Volatility:  {vol_stats['min']:.4f}\")\n",
    "    print(f\"  Max Volatility:  {vol_stats['max']:.4f}\")\n",
    "    print(f\"  Std Volatility:  {vol_stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0ebe0",
   "metadata": {},
   "source": [
    "## 5. Returns Distribution and Risk Metrics\n",
    "Analyze return distributions and calculate risk metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48b71de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Metrics Summary:\n",
      "================================================================================\n",
      "\n",
      "AAPL:\n",
      "  Mean Daily Return:     0.001161\n",
      "  Daily Volatility:      0.021147\n",
      "  Annualized Volatility: 0.3357\n",
      "  Skewness:              0.0790\n",
      "  Kurtosis:              4.8794\n",
      "  5% VaR:                -0.032406\n",
      "  1% VaR:                -0.055592\n",
      "  Sharpe Ratio:          0.8712\n",
      "  Maximum Drawdown:      -0.314273\n",
      "\n",
      "MSFT:\n",
      "  Mean Daily Return:     0.001058\n",
      "  Daily Volatility:      0.020555\n",
      "  Annualized Volatility: 0.3263\n",
      "  Skewness:              0.0266\n",
      "  Kurtosis:              6.4922\n",
      "  5% VaR:                -0.029482\n",
      "  1% VaR:                -0.049500\n",
      "  Sharpe Ratio:          0.8170\n",
      "  Maximum Drawdown:      -0.375565\n",
      "\n",
      "GOOGL:\n",
      "  Mean Daily Return:     0.000934\n",
      "  Daily Volatility:      0.021124\n",
      "  Annualized Volatility: 0.3353\n",
      "  Skewness:              -0.0947\n",
      "  Kurtosis:              3.2273\n",
      "  5% VaR:                -0.032451\n",
      "  1% VaR:                -0.054304\n",
      "  Sharpe Ratio:          0.7015\n",
      "  Maximum Drawdown:      -0.443201\n",
      "\n",
      "AMZN:\n",
      "  Mean Daily Return:     0.000750\n",
      "  Daily Volatility:      0.023741\n",
      "  Annualized Volatility: 0.3769\n",
      "  Skewness:              0.0980\n",
      "  Kurtosis:              3.7625\n",
      "  5% VaR:                -0.035522\n",
      "  1% VaR:                -0.059371\n",
      "  Sharpe Ratio:          0.5014\n",
      "  Maximum Drawdown:      -0.561453\n",
      "\n",
      "TSLA:\n",
      "  Mean Daily Return:     0.003070\n",
      "  Daily Volatility:      0.042902\n",
      "  Annualized Volatility: 0.6810\n",
      "  Skewness:              0.0745\n",
      "  Kurtosis:              2.8600\n",
      "  5% VaR:                -0.064069\n",
      "  1% VaR:                -0.115484\n",
      "  Sharpe Ratio:          1.1359\n",
      "  Maximum Drawdown:      -0.736322\n"
     ]
    }
   ],
   "source": [
    "# Analyze return distributions\n",
    "fig, axes = plt.subplots(2, len(enhanced_data), figsize=(4*len(enhanced_data), 8))\n",
    "if len(enhanced_data) == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i, (ticker, df) in enumerate(enhanced_data.items()):\n",
    "    # Histogram of daily returns\n",
    "    axes[0, i].hist(df['daily_return'].dropna(), bins=50, alpha=0.7, density=True)\n",
    "    axes[0, i].axvline(df['daily_return'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, i].set_title(f'{ticker} - Daily Returns Distribution')\n",
    "    axes[0, i].set_xlabel('Daily Return')\n",
    "    axes[0, i].set_ylabel('Density')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(df['daily_return'].dropna(), dist=\"norm\", plot=axes[1, i])\n",
    "    axes[1, i].set_title(f'{ticker} - Q-Q Plot (Normality Check)')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate risk metrics\n",
    "print(\"\\nRisk Metrics Summary:\")\n",
    "print(\"=\"*80)\n",
    "for ticker, df in enhanced_data.items():\n",
    "    returns = df['daily_return'].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    skewness = returns.skew()\n",
    "    kurtosis = returns.kurtosis()\n",
    "    \n",
    "    # Risk metrics\n",
    "    var_95 = returns.quantile(0.05)  # 5% VaR\n",
    "    var_99 = returns.quantile(0.01)  # 1% VaR\n",
    "    \n",
    "    # Sharpe ratio (assuming 0% risk-free rate)\n",
    "    sharpe_ratio = mean_return / std_return * np.sqrt(252) if std_return != 0 else 0\n",
    "    \n",
    "    # Maximum drawdown calculation\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative_returns.expanding().max()\n",
    "    drawdown = (cumulative_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Mean Daily Return:     {mean_return:.6f}\")\n",
    "    print(f\"  Daily Volatility:      {std_return:.6f}\")\n",
    "    print(f\"  Annualized Volatility: {std_return * np.sqrt(252):.4f}\")\n",
    "    print(f\"  Skewness:              {skewness:.4f}\")\n",
    "    print(f\"  Kurtosis:              {kurtosis:.4f}\")\n",
    "    print(f\"  5% VaR:                {var_95:.6f}\")\n",
    "    print(f\"  1% VaR:                {var_99:.6f}\")\n",
    "    print(f\"  Sharpe Ratio:          {sharpe_ratio:.4f}\")\n",
    "    print(f\"  Maximum Drawdown:      {max_drawdown:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4fa970",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "Analyze correlations between different stocks for portfolio risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c83f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Analysis:\n",
      "==================================================\n",
      "\n",
      "Returns Correlation Matrix:\n",
      "         AAPL    MSFT   GOOGL    AMZN    TSLA\n",
      "AAPL   1.0000  0.7768  0.6911  0.6239  0.5111\n",
      "MSFT   0.7768  1.0000  0.7723  0.6793  0.4716\n",
      "GOOGL  0.6911  0.7723  1.0000  0.6643  0.4282\n",
      "AMZN   0.6239  0.6793  0.6643  1.0000  0.4542\n",
      "TSLA   0.5111  0.4716  0.4282  0.4542  1.0000\n",
      "\n",
      "Volatility Correlation Matrix:\n",
      "         AAPL    MSFT   GOOGL    AMZN    TSLA\n",
      "AAPL   1.0000  0.8960  0.7377  0.6007  0.7372\n",
      "MSFT   0.8960  1.0000  0.8727  0.6845  0.6809\n",
      "GOOGL  0.7377  0.8727  1.0000  0.7765  0.5475\n",
      "AMZN   0.6007  0.6845  0.7765  1.0000  0.4502\n",
      "TSLA   0.7372  0.6809  0.5475  0.4502  1.0000\n",
      "\n",
      "Average Returns Correlation: 0.6073\n",
      "Average Volatility Correlation: 0.6984\n"
     ]
    }
   ],
   "source": [
    "# Create correlation analysis\n",
    "if len(enhanced_data) > 1:\n",
    "    # Create a combined dataframe of returns\n",
    "    returns_df = pd.DataFrame()\n",
    "    prices_df = pd.DataFrame()\n",
    "    volatility_df = pd.DataFrame()\n",
    "    \n",
    "    for ticker, df in enhanced_data.items():\n",
    "        returns_df[ticker] = df['daily_return']\n",
    "        prices_df[ticker] = df['close']\n",
    "        volatility_df[ticker] = df['volatility_20']\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    returns_corr = returns_df.corr()\n",
    "    vol_corr = volatility_df.corr()\n",
    "    \n",
    "    # Plot correlation heatmaps\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Returns correlation\n",
    "    sns.heatmap(returns_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "    axes[0].set_title('Daily Returns Correlation Matrix')\n",
    "    \n",
    "    # Volatility correlation\n",
    "    sns.heatmap(vol_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "    axes[1].set_title('Volatility Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation statistics\n",
    "    print(\"Correlation Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nReturns Correlation Matrix:\")\n",
    "    print(returns_corr.round(4))\n",
    "    print(\"\\nVolatility Correlation Matrix:\")\n",
    "    print(vol_corr.round(4))\n",
    "    \n",
    "    # Calculate average correlations\n",
    "    returns_avg_corr = returns_corr.values[np.triu_indices_from(returns_corr.values, k=1)].mean()\n",
    "    vol_avg_corr = vol_corr.values[np.triu_indices_from(vol_corr.values, k=1)].mean()\n",
    "    \n",
    "    print(f\"\\nAverage Returns Correlation: {returns_avg_corr:.4f}\")\n",
    "    print(f\"Average Volatility Correlation: {vol_avg_corr:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Correlation analysis requires multiple stocks. Only one stock loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80353b4",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "Save the enhanced dataset to interim folder for further feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e964943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to interim folder...\n",
      "Directory: /workspaces/Systematic-Options-Auto-Hedging-Engine/data/interim\n",
      "âœ“ Saved AAPL data to AAPL_processed_20251001.csv (1006 records)\n",
      "âœ“ Saved MSFT data to MSFT_processed_20251001.csv (1006 records)\n",
      "âœ“ Saved GOOGL data to GOOGL_processed_20251001.csv (1006 records)\n",
      "âœ“ Saved AMZN data to AMZN_processed_20251001.csv (1006 records)\n",
      "âœ“ Saved TSLA data to TSLA_processed_20251001.csv (1006 records)\n",
      "\n",
      "âœ“ Saved EDA summary to eda_summary_20251001.csv\n",
      "\n",
      "Processed Data Summary:\n",
      "  ticker  records  start_date    end_date  mean_return  volatility  \\\n",
      "0   AAPL     1006  2020-01-02  2023-12-29     0.001161    0.144101   \n",
      "1   MSFT     1006  2020-01-02  2023-12-29     0.001058    0.146163   \n",
      "2  GOOGL     1006  2020-01-02  2023-12-29     0.000934    0.254851   \n",
      "3   AMZN     1006  2020-01-02  2023-12-29     0.000750    0.189638   \n",
      "4   TSLA     1006  2020-01-02  2023-12-29     0.003070    0.334480   \n",
      "\n",
      "   max_drawdown  \n",
      "0      0.758981  \n",
      "1      0.802266  \n",
      "2      0.970402  \n",
      "3      1.103793  \n",
      "4     10.523986  \n"
     ]
    }
   ],
   "source": [
    "# Save processed data to interim folder\n",
    "interim_dir = os.path.join(project_root, \"data\", \"interim\")\n",
    "os.makedirs(interim_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving processed data to interim folder...\")\n",
    "print(f\"Directory: {interim_dir}\")\n",
    "\n",
    "for ticker, df in enhanced_data.items():\n",
    "    filename = f\"{ticker}_processed_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    filepath = os.path.join(interim_dir, filename)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filepath)\n",
    "    print(f\"âœ“ Saved {ticker} data to {filename} ({len(df)} records)\")\n",
    "\n",
    "# Create a summary report\n",
    "summary_data = []\n",
    "for ticker, df in enhanced_data.items():\n",
    "    summary_data.append({\n",
    "        'ticker': ticker,\n",
    "        'records': len(df),\n",
    "        'start_date': df.index.min().strftime('%Y-%m-%d'),\n",
    "        'end_date': df.index.max().strftime('%Y-%m-%d'),\n",
    "        'mean_return': df['daily_return'].mean(),\n",
    "        'volatility': df['volatility_20'].iloc[-1],\n",
    "        'max_drawdown': ((1 + df['daily_return'].fillna(0)).cumprod().cummax() - (1 + df['daily_return'].fillna(0)).cumprod()).max()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_filepath = os.path.join(interim_dir, f\"eda_summary_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "summary_df.to_csv(summary_filepath, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Saved EDA summary to eda_summary_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "print(\"\\nProcessed Data Summary:\")\n",
    "print(summary_df.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713aad5",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This EDA notebook has successfully:\n",
    "\n",
    "1. **Loaded raw data** using the improved data_loader module with fallback providers\n",
    "2. **Analyzed basic statistics** and data quality for multiple stocks\n",
    "3. **Calculated returns and momentum indicators** for risk assessment\n",
    "4. **Examined volatility patterns** and clustering behavior\n",
    "5. **Analyzed return distributions** and calculated comprehensive risk metrics\n",
    "6. **Performed correlation analysis** between different assets\n",
    "7. **Saved processed data** to interim folder for further analysis\n",
    "\n",
    "### Key Findings:\n",
    "- All stocks show realistic volatility patterns with clustering\n",
    "- Return distributions show expected fat tails (high kurtosis)\n",
    "- Correlations between tech stocks are generally positive but not perfect\n",
    "- Risk metrics are within expected ranges for equity markets\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the processed data in `data/interim/` for options Greeks validation (notebook 02)\n",
    "2. Implement hedge performance analysis (notebook 03)\n",
    "3. Develop dynamic hedging strategies based on volatility regimes\n",
    "4. Backtest the systematic hedging engine with real market data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434a53",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting Section\n",
    "Quick tests to verify the notebook environment is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a81d0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Testing Notebook Environment\n",
      "========================================\n",
      "âœ“ Core data science libraries imported\n",
      "âœ“ Data loader module accessible\n",
      "  Available functions: ['EmptyDataError', 'load_data', 'main', 'obb', 'pd']...\n",
      "âœ“ Basic pandas operations working\n",
      "\n",
      "ðŸŽ‰ Environment test completed!\n",
      "If all tests passed, the notebook should work correctly.\n"
     ]
    }
   ],
   "source": [
    "# Quick environment test\n",
    "print(\"ðŸ”§ Testing Notebook Environment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test 1: Basic imports\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"âœ“ Core data science libraries imported\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Core libraries failed: {e}\")\n",
    "\n",
    "# Test 2: Data loader\n",
    "try:\n",
    "    # Test a simple function call\n",
    "    print(\"âœ“ Data loader module accessible\")\n",
    "    print(f\"  Available functions: {[f for f in dir(data_loader) if not f.startswith('_')][:5]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Data loader test failed: {e}\")\n",
    "\n",
    "# Test 3: Simple data operation\n",
    "try:\n",
    "    test_df = pd.DataFrame({'test': [1, 2, 3]})\n",
    "    result = test_df.mean()\n",
    "    print(\"âœ“ Basic pandas operations working\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pandas operations failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Environment test completed!\")\n",
    "print(\"If all tests passed, the notebook should work correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
